"Highly correlated to correctly producing corresponding segments of the output, attentions can dramatically improve performance.\n\nDissecting a Neural Translation Network\n\nState-of-the-art neural translation networks use a number of different techniques and advancements that build on the basic sequence-to-sequence (seq2seq) encoder-decoder architecture. Attention, as detailed in the previous section, is an important and critical architectural improvement. In this section, we will dissect a fully implemented neural machine translation system, training it, and eventually using it to convert English phrases to French phrases! We will pursue this exploration by working with a simplified version of the official TensorFlow machine translation tutorial code.\n\nThe pipeline used in training and eventually using a neural machine translation system is very similar to that of most machine learning pipelines: gather data, prepare the data, construct the model, train the model, evaluate the model's progress, and eventually use the trained model to predict or infer something useful. We review each of these steps here.\n\nWe first gather the data from the WMT'15 repository, which houses large corpora used in training translation systems. For our use case, we will be using the English-to-French data. Note that if we want to translate to or from different languages, we would have to train a model from scratch with the new data. We then preprocess our data into a format that is easily usable by our models during training and inference time. This will involve some amount of cleaning and tokenizing the sentences in each of the English and French phrases. What follows now is a set of techniques used in preparing the data, and later we will present the implementations of these techniques.\n\nThe first step is to parse sentences and phrases into formats that are more compatible with the model by tokenization. This is the process by which we discretize a particular English or French sentence into its constituent tokens. For instance, a simple word-level tokenizer will consume the sentence \"I read.\" to produce the array [\"I\", \"read\"], or it would consume the French sentence \"Je lis.\" to produce the array [\"Je\", \"lis\"]. A character-level tokenizer may break the sentence into individual characters or into pairs of characters like [\"I\", \"r\", \"e\", \"a\", \"d\"]. One kind of tokenization may work better than the other, and each has its pros and cons. For instance, a word-level tokenizer will ensure that the model produces words that are from some dictionary, but the size of the dictionary may be too large to efficiently choose from during decoding. This is, in fact, a known issue.We appended End Of Sequence (EOS) tokens to our target sequences. \n\nTarget sequences: \n- peas \n- <PAD> \n- <PAO> \n- <PAD> \n- <PAD> \n- je \n- lis \n- (EOS) \n- (PAD) \n- (PAD) \n- KPAGY \n- See \n- you \n- in \n- little \n- while \n- tout \n- 1'heure \n\nThis step saves us the trouble of having to construct a different sequence-to-sequence (seq2seq) model for each pair of source and target lengths. However, this introduces a different issue: if there is a very long sequence, we would have to pad every other sequence up to that length. This means that a short sequence padded to the maximum length would take as much computational resources as a long one with few PAD tokens, which is wasteful and could introduce a major performance hit to our model. \n\nReference: Dissecting a Neural Translation NetworkThe corresponding translations are where bucketing helps us, as shown in Figure 7-29.\n\nBucket 1:\n- Je\n- little\n- in\n- while\n- See\n- you\n- l'heure\n\nBucket 2:\n- tout\n\nFigure 7-29: Padding sequences with buckets.\n\nThese methods allow for even further GPU efficiency. With the sequences properly padded, we need to add one additional token to the target sequences: a GO token. This GO token will signal to the decoder that decoding needs to begin, at which point it will take over and start the decoding process.\n\nThe last improvement we make in the data preparation phase is reversing the order of the input sequences. This has become a standard trick to try when training neural machine translation models. Although it may seem like an engineering hack, consider that our fixed-size neural state can only hold a limited amount of information. Information encoded while processing the beginning of the sentence may be overwritten while encoding later parts.Read\nJe lis\n\nBucket 1\nWhile little in\nTout 500\nL'heure\n\nBucket j\n\nSets and reversing the inputs:\n\nFor _ in range(self.batch_size):\n    bucket_id]\n\n# Encoder inputs are padded and then reversed.\nencoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\nencoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n\n# Decoder inputs get an extra 'Go' symbol,\n# and are then padded.\ndecoder_pad_size = decoder_size - len(decoder_input) - 1\ndecoder_inputs.append([data_utils.GO_ID] + decoder_input + [data_utils.PAD_ID] * decoder_pad_size)\n\n# Above.\n[], [], []\n\nDissecting a Neural Translation NetworkFor length_idx in range(encoder_size):\n    batch_encoder_inputs.append(\n        np.array([\n            encoder_inputs[batch_idx][length_idx]\n            for batch_idx in range(self.batch_size)\n        ], dtype=np.int32)\n    )\n\n# Batch decoder inputs are re-indexed decoder_inputs, we create weights.\nfor length_idx in range(decoder_size):\n    batch_decoder_inputs.append(\n        np.array([\n            decoder_inputs[batch_idx][length_idx]\n            for batch_idx in range(self.batch_size)\n        ], dtype=np.int32)\n    )\n\n# Defined placeholders into the correct shape:\n# Create target_weights to be 0 for targets that are padding.\nbatch_weight = np.ones(self.batch_size, dtype=np.float32)\nfor batch_idx in range(self.batch_size):\n    # We set weight to 0 if the corresponding target is a PAD symbol.\n    # The corresponding target is decoder_input shifted by 1 forward.\n    if length_idx < decoder_size - 1:\n        target = decoder_inputs[batch_idx][length_idx + 1]\n        if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\n            batch_weight[batch_idx] = 0.0\n    batch_weights.append(batch_weight)\nreturn batch_encoder_inputs, batch_decoder_inputs, batch_weights\n\n# Finally, we set the target weights of zero to those tokens that are simply the PAD token.\n# With the data preparation now done, we are ready to begin building and training our model. \n# We first detail the code used during training and test time, and abstract the model pipeline.\n# The first step during training is to load our data:\n\ndef train():\n    # Prepare WMT data.\n    print(\"Preparing WMT data in %s\" % FLAGS.data_dir)\n    en_train, fr_train, en_dev, fr_dev, _, _ = data_utils.prepare_wmt_data(\n        FLAGS.data_dir, FLAGS.en_vocab_size, FLAGS.fr_vocab_size\n    )with tf.Session() as sess:\n    # Create model.\n    FLAGS.size = ...\n    ties = % FLAGS.max_train_data_size\n    dev_set = read_data(en_dev, fr_dev)\n    train_set = read_data(en_train, fr_train, FLAGS.max_train_data_size)\n    train_bucket_sizes = [len(train_set[b]) for b in range(len(_buckets))]\n    train_total_size = float(sum(train_bucket_sizes))\n    # A bucket scale is a list of increasing numbers\n    # from 0 to 1 that we'll use to select a bucket.\n    # Length of [scale[i], scale[i+1]] is proportional to\n    # the size of the i-th training bucket, as used later.\n    train_buckets_scale = [sum(train_bucket_sizes[:i+1]) / train_total_size for i in range(len(train_bucket_sizes))]\n    # Initialize variables like current_step and previous_losses to 0 or empty.\n    current_step = 0\n    previous_losses = []\n    # This is the training loop.\n    step_time, loss = 0.0, 0.0\n    while True:\n        # We pick a random number\n        # in train_buckets_scale.random_number_01 = np.random.random_sample()\nbucket_id = min([i for i in range(len(train_buckets_scale)) if train_buckets_scale[i] > random_number_01])\n\n# Get a batch and make a step.\nstart_time = time.time()\nmodel.get_batch(train_set, bucket_id)\ndecoder_inputs, target_weights, bucket_id, False)\n\n# Running metrics:\nstep_time += (time.time() - start_time) / FLAGS.steps_per_checkpoint\nloss += step_loss / FLAGS.steps_per_checkpoint\ncurrent_step += 1\n\n# Once in a while, we save checkpoint, print statistics, and run evaluations.\nif current_step % FLAGS.steps_per_checkpoint == 0:\n    # Print statistics for the previous epoch.\n    perplexity = math.exp(float(loss)) if loss < 300 else float(\"inf\")\n    print(\"global step %d learning rate %.4f step-time %.2f perplexity %.2f\" % (model.global_step.eval(), model.learning_rate.eval(), step_time, perplexity))\n\n# Decrease learning rate if no improvement was seen over the last 3 times.\nif len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n    sess.run(model.learning_rate_decay_op)\nprevious_losses.append(loss)\n\n# Save checkpoint and zero timer and loss.\ncheckpoint_path = os.path.join(FLAGS.train_dir, \"translate.ckpt\")\nmodel.saver.save(sess, checkpoint_path)global_step = model.global_step\n# their perplexity.\nif len(dev_set[bucket_id]) == 0:\n    continue\nencoder_inputs, decoder_inputs, target_weights = model.get_batch(dev_set, bucket_id)\nencoder_inputs, decoder_inputs, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\neval_ppx = math.exp(float(eval_loss)) if eval_loss < 300 else float('inf')\nprint('eval: bucket %d perplexity %.2f' % (bucket_id, eval_ppx))\nsys.stdout.flush()\n\nWe also have another major use case for our model: single-use prediction. In this case, we, or other users, provide input data. To do so, we use the decode() method. This method will essentially carry out the same functions as were done in the evaluation loop for the held-out development set. However, the largest difference is that during training and evaluation, we never needed the model to translate the output embeddings to output tokens that are human-readable, which is something we do here. We detail this method now.\n\npoint step:\ndef decode():\n    with tf.Session() as sess:Check if the sizes match.\n\nencoder_size, decoder_size = self.buckets[bucket_id]\n\nif len(encoder_inputs) != encoder_size:\n    raise ValueError(\"Encoder length must be equal to the one in bucket, %d != %d.\" % (len(encoder_inputs), encoder_size))\n\nif len(decoder_inputs) != decoder_size:\n    raise ValueError(\"Decoder length must be equal to the one in bucket, %d != %d.\" % (len(decoder_inputs), decoder_size))\n\nif len(target_weights) != decoder_size:\n    raise ValueError(\"Weights length must be equal to the one in bucket, %d != %d.\" % (len(target_weights), decoder_size))\n\n# Input feed: encoder inputs, decoder inputs, target_weights, as provided.\ninput_feed = {}\n\nfor l in range(encoder_size):\n    input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n\nfor l in range(decoder_size):\n    input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n    input_feed[self.target_weights[l].name] = target_weights[l]\n\n# Since our targets are decoder inputs shifted by one, we need one more.\nlast_target = self.decoder_inputs[decoder_size].name\ninput_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n\n# The output feed, if a loss is computed and needs to be backpropagated through the network and computes the gradient norm and loss for the batch:\n# Output feed: depends on whether we do a backward step or not.\nif not forward_only:\n    output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n                   self.gradient_norms[bucket_id],  # Gradient norm.\n                   self.losses[bucket_id]]  # Loss for this batch.\nelse:\n    output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n\nfor l in range(decoder_size):  # Output logits.\n    output_feed.append(self.outputs[bucket_id][l])Outputs are returned for decoding purposes:\n\noutputs = session.run(output_feed, input_feed)\n\nif not forward_only:\n    return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\nelse:\n    return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n\nThe create_model() method itself is fairly straightforward. It uses a number of user-defined or default flags, such as the sizes of the English and French vocabularies and batch size, to create the model by using the constructor seq2seq_model.Seq2SeqModel. One particularly interesting flag is the use_fp16 flag. With this, a lower precision is used as the type in the underlying numpy arrays. This results in faster performance at the cost of some amount of precision. However, it's often the case that 16-bit representations are sufficient for representing losses and gradient updates and often perform close to the level of using 32-bit representations.\n\nModel creation can be achieved using the following code:\n\ndef create_model(session, forward_only):\n    \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n    dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n    model = seq2seq_model.Seq2SeqModel(\n        FLAGS.en_vocab_size,\n        FLAGS.fr_vocab_size,\n        _buckets,\n        FLAGS.size,\n        FLAGS.num_layers,\n        FLAGS.max_gradient_norm,\n        FLAGS.batch_size,\n        FLAGS.learning_rate,\n        FLAGS.learning_rate_decay_factor,\n        forward_only=forward_only,\n        dtype=dtype)\n\nBefore returning the model, a check is done to see if there are any previously checkpointed models from earlier training runs. If so, this model and its parameters are read into the model variable and used. This allows us to stop training at a checkpoint and later resume it without training from scratch. Otherwise, the fresh model created is returned as the main object.ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\nif ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n    print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n    model.saver.restore(session, ckpt.model_checkpoint_path)\nelse:\n    print(\"Created model with fresh parameters.\")\n    session.run(tf.global_variables_initializer())\nreturn model\n\nThe code sketches the details of the overarching computation graph. A few class-level fields are created:\n\nclass Seq2SeqModel(object):\n    def __init__(self,\n        source_vocab_size,\n        target_vocab_size,\n        buckets,\n        size,\n        num_layers,\n        max_gradient_norm,\n        batch_size,\n        learning_rate,\n        learning_rate_decay_factor,\n        use_lstm=False,\n        num_samples=512,\n        forward_only=False,\n        dtype=tf.float32):\n        self.source_vocab_size = source_vocab_size\n        self.target_vocab_size = target_vocab_size\n        self.buckets = buckets\n        self.batch_size = batch_size\n        self.learning_rate = tf.Variable(float(learning_rate), trainable=False, dtype=dtype)\n        self.learning_rate_decay_op = self.learning_rate.assign(self.learning_rate * learning_rate_decay_factor)\n        self.global_step = tf.Variable(0, trainable=False)\n\nThe next part creates the sampled softmax and the output projection. This is an improvement over basic seq2seq models in that they allow for efficient decoding.Vocabulary Size:\nself.target_vocab_size:\nstze],dtyre=dtyre)\nw = tf.transpose(w_t)\ndtype=dtype)\noutput_projection = (w, b)\nlocal_b = tf.cast(b, tf.float32)\nreturn tf.cast(\ntf.n.sampled_softmax_loss(local_w_t, local_b,\nlocal_inputs, labels,\nnum_samples,\nself.target_vocab_size),\ndtype)\nsoftmax_loss_function = sampled_loss\n\n# Create the internal multi-layer cell for our RNN.\nsingle_cell = tf.nn.rnn_cell.GRUCell(size)\nif use_lstm:\n    single_cell = tf.nn.rnn_cell.BasicLSTMCell(size)\ncell = single_cell\nif num_layers > 1:\n    cell = tf.nn.rnn_cell.MultiRNNCell([single_cell] * num_layers)\n\n# Input and attention.\nencoder_inputs,\ndecoder_inputs,\nDissecting a Neural Translation Network | 207In this document, we define the parameters for a sequence-to-sequence model. The parameters include the number of encoder symbols, the number of decoder symbols, the embedding size, the output projection, and the data type. \n\nWe then define placeholders for the inputs and targets:\n\n- Encoder Inputs: A list to hold the encoder inputs.\n- Decoder Inputs: A list to hold the decoder inputs.\n- Target Weights: A list to hold the target weights.\n\nFor the encoder inputs, we iterate through the last bucket, which is the biggest one, to create placeholders for each encoder input:\n\n```python\nfor i in range(buckets[-1][0]):\n    self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=\"encoder{0}\".format(i)))\n```\n\nFor the decoder inputs, we create placeholders for each input, including an extra one for the start of the sequence:\n\n```python\nfor i in range(buckets[-1][1] + 1):\n    self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None], name=\"decoder{0}\".format(i)))\n    self.target_weights.append(tf.placeholder(dtype, shape=[None], name=\"weight{0}\".format(i)))\n```\n\nOur targets are the decoder inputs shifted by one position:\n\n```python\ntargets = [self.decoder_inputs[i + 1] for i in range(len(self.decoder_inputs) - 1)]\n```\n\nNext, we compute the outputs and losses from the model. If we are in training mode (i.e., forward_only is false), we use the following code to calculate the outputs and losses:\n\n```python\nif forward_only:\n    self.outputs, self.losses = seq2seq.model_with_buckets(\n        self.encoder_inputs, self.decoder_inputs, targets,\n        self.target_weights, buckets, lambda x, y: seq2seq_f(x, y, True),\n        softmax_loss_function=softmax_loss_function)\n```\n\nIf output projection is specified, we project the outputs for decoding:\n\n```python\nif output_projection is not None:\n    for b in range(len(buckets)):\n        self.outputs[b] = [tf.matmul(output, output_projection[0]) + output_projection[1] for output in self.outputs[b]]\n```\n\nThis section of the document illustrates the setup for a sequence-to-sequence model, including input handling and loss computation.If not forward_only:\n    self.gradient_norms = []\n    self.updates = []\n    self.learning_rate\n    for b in range(len(buckets)):\n        gradients,\n        max_gradient_norm\n        self.gradient_norms.append(norm)\n        self.updates.append(opt.apply_gradients(\n            zip(clipped_gradients, params), global_step=self.global_step))\n    self.saver = tf.train.Saver(tf.all_variables())\n\nWhen initializing this model, several flags and arguments are passed as function arguments. One argument of particular note is feed_previous. When this is true, the decoder will use the outputted logit at time step T as input to time step T+1. In this way, it is sequentially decoding the next token based on all tokens thus far. We can describe this type of decoding, where the next output depends on all previous outputs, as autoregressive decoding.\n\ndef embedding_attention_seq2seq(encoder_inputs,\n    decoder_inputs,\n    cell,\n    num_encoder_symbols,\n    num_decoder_symbols,\n    embedding_size,\n    output_projection=None,\n    feed_previous=False,\n    dtype=None,Scope: None\nInitial State Attention: False\n\nWe first create the wrapper for the encoder.\n\nWith variable_scope.variable_scope(as scope):\n    dtype = scope.dtype\n    encoder_cell = rnn_cell.EmbeddingWrapper(\n        cell,\n        embedding_classes=num_encoder_symbols,\n        embedding_size=embedding_size)\n    encoder_outputs, encoder_state = rnn.rnn(\n        encoder_cell, encoder_inputs, dtype=dtype)\n\n# As a distribution:\n# First calculate a concatenation of encoder outputs to put attention on.\n    top_states = [\n        array_ops.reshape(e, [-1, 1, cell.output_size]) for e in encoder_outputs\n    ]\n    attention_states = array_ops.concat(1, top_states)\n\n# Wrapped to be one that uses an output projection:\n    output_size = None\n    if output_projection is None:\n        cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n        output_size = num_decoder_symbols\n\nFrom here, we compute the outputs and states using the embedding_attention_decoder:\n    if isinstance(feed_previous, bool):\n        return embedding_attention_decoder(\n            decoder_inputs,\n            encoder_state,\n            attention_states,\n            cell,\n            num_decoder_symbols,\n            embedding_size,\n            output_size=output_size,\n            output_projection=output_projection,\n            feed_previous=feed_previous,\n            initial_state_attention=initial_state_attention)\n\nChapter 7: Models for Sequence AnalysisInvoked in this step: \n\n- initial_state \n- attention_states \n- cell \n- num_symbols \n- embedding_size \n- output_size (default None) \n- output_projection (default None) \n- feed_previous (default False) \n- update_embedding_for_previous (default True) \n- dtype (default None) \n- scope (default None) \n- initial_state_attention (default False) \n\nIf output_size is None: \n    output_size = cell.output_size \n\nIf output_projection is not None: \n    dtype = dtype \n\nWith variable_scope.variable_scope(scope or \"embedding_attention_decoder\", dtype=dtype) as scope: \n    embedding = variable_scope.get_variable(\"embedding\", [num_symbols, embedding_size]) \n    loop_function = _extract_argmax_and_embed(embedding, output_projection, update_embedding_for_previous) if feed_previous else None \n    emb_inp = [embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs] \n\nReturn attention_decoder( \n    emb_inp, \n    initial_state, \n    attention_states, \n    cell, \n    output_size=output_size \n) \n\nDissecting a Neural Translation Network | 211loop_function = loop_function,\ninitial_state_attention = initial_state_attention)\n\n# The hidden features to the right size:\ndef attention_decoder(decoder_inputs,\n    initial_state,\n    attention_states,\n    cell,\n    output_size=None,\n    loop_function=None,\n    dtype=None,\n    scope=None,\n    initial_state_attention=False):\n    if not decoder_inputs:\n        raise ValueError(\"decoder_inputs cannot be empty.\")\n    if attention_states.get_shape()[2].value is None:\n        raise ValueError(\"Shape[2] of attention_states must be known: %s\" %\n                         attention_states.get_shape())\n    if output_size is None:\n        output_size = cell.output_size\n    with variable_scope.variable_scope(scope or \"attention_decoder\", dtype=dtype) as scope:\n        dtype = scope.dtype\n        batch_size = array_ops.shape(decoder_inputs[0])[0]  # Needed for reshaping.\n        attn_length = attention_states.get_shape()[1].value\n        if attn_length is None:\n            attn_length = array_ops.shape(attention_states)[1]\n        attn_size = attention_states.get_shape()[2].value\n        # To calculate W1 * h_t we use a 1-by-1 convolution,\n        # need to reshape before.\n        hidden = array_ops.reshape(attention_states, [-1, attn_length, 1, attn_size])\n        hidden_features = []\n        v = []\n        attention_vec_size = attn_size  # Size of query vectors\n        for attention in range(attn_length):\n            k = variable_scope.get_variable(\"AttnW_o\",\n                [1, 1, attn_size, attention_vec_size])\n            hidden_features.append(nn_ops.conv2d(hidden, k, [1, 1, 1, 1], \"SAME\"))\n\n# Chapter 7 Models for Sequence Analysisv.append([attention_vec_size])\nstate = initial_state\n\ndef attention(query):\n    # stored here.\n    # flatten it.\n    # specified.\n    ndims = query.get_shape().ndims\n    if ndims:\n        assert ndims == 2\n    query = array_ops.concat(1, query_list)\n    # query = array_ops.concat(query_list, 1)\n    y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n    # Attention mask is a softmax of v^T * tanh(...).\n    s = math_ops.reduce_sum(v[e] * math_ops.tanh(hidden_features[e] + y), [2, 3])\n    a = nn_ops.softmax(s)\n    # Now calculate the attention-weighted vector d.\n    d = math_ops.reduce_sum(array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden, [1, 2])\n    ds.append(array_ops.reshape(d, [-1, attn_size]))\n    return ds\n\nwith the initial state:\n    outputs = []\n    prev = None\n    # vectors is set.\n    a.set_shape([None, attn_size])\n    if initial_state_attention:\n        # (ese)oae = se\nDissecting a Neural Translation Network | 213To the same dynamics:\n\nFor i, inp in enumerate(decoder_inputs):\n    if t > 0:\n        # If loop_function is set, we use it instead of decoder_inputs.\n        if loop_function is not None and prev is not None:\n            with variable_scope.variable_scope(\"loop_function\", reuse=True):\n                inp = loop_function(prev, t)\n\n    # Merge input and previous attentions into one vector of the right size.\n    input_size = inp.get_shape().with_rank(2)[1]\n    if input_size.value is None:\n        raise ValueError(\"could not infer input size from input: %s\" % inp.name)\n    x = linear([inp] + attns, input_size, True)\n\n    # Run the RNN.\n    cell_output, state = cell(x, state)\n\n    # Run the attention mechanism.\n    if i == 0 and initial_state_attention:\n        with variable_scope.variable_scope(variable_scope.get_variable_scope(), reuse=True):\n            attns = attention(state)\n    else:\n        attns = attention(state)\n\n    with variable_scope.variable_scope(\"AttnoutputProjection\"):\n        output = linear([cell_output] + attns, output_size, True)\n\n    if loop_function is not None:\n        prev = output\n    outputs.append(output)\n\nreturn outputs, state\n\nWith this, we have successfully completed a full tour of the implementation details of a fairly sophisticated neural machine translation system. Production systems have compute servers to ensure that state-of-the-art performance is met, along with the learning rate annealing over time as well.Perplexity for Regression French Translation\n\n400\n200\n8\n600\n400\n15k\n20k\n25k\n30k\n10k\n5k\nEpochs\n\nLearning Rate Decay\n\n100k\n150k\n200k\n250k\n300k\n500k\nEpochs\n\nDuring training, the model was approaching a stable state.\n\nDissecting a Neural Translation NetworkWhen translating the 'i' token in the French sentence, we can immediately see that the attention mechanism seems to be working quite well. Large amounts of attention are generally being placed in the right areas, even though there is slight noise in the model's prediction. It is possible that adding additional layers to the network would help produce crisper attention. One impressive aspect is that the phrase 'the European Economic' is translated in reverse in French as 'la zone \u00e9conomique europ\u00e9enne,' and as such, the attention weights reflect this flip! These kinds of attention patterns may be even more interesting when translating from English to a different language that does not parse smoothly from left to right. This works and begins a foray into more sophisticated learning.Summary of transcription."