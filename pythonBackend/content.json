{
    "content": [
        "highly correlated to correctly producing corresponding segments of the output, atten-\ntions can dramatically improve performance.\nDissecting a Neural Translation Network\nState-of-the-art neural translation networks use a number of different techniques and\nadvancements that build on the basic seq2seq encoder-decoder architecture. Atten-\ntion, as detailed in the previous section, is an important and critical architectural\nimprovement. In this section, we will dissect a fully implemented neural machine\ntraining it, and eventually using it as a translation system to convert English phrases\nto French phrases! We'll pursue this exploration by working with a simplified version\nof the official TensorFlow machine translation tutorial code.2\nThe pipeline used in training and eventually using a neural machine translation sys-\ntem is very similar to that of most machine learning pipelines: gather data, prepare\nthe data, construct the model, train the model, evaluate the model's progress, and\neventually use the trained model to predict or infer something useful. We review each\nof these steps here.\nWe first gather the data from the WMT'15 repository, which houses large corpora\nused in training translation systems. For our use case, we'll be using the English-to-\nFrench data. Note that if we want to be able to translate to or from different lan-\nguages, we would have to train a model from scratch with the new data. We then\npreprocess our data into a format that is easily usable by our models during training\nand inference time. This will involve some amount of cleaning and tokenizing the\nsentences in each of the English and French phrases. What follows now is a set of\ntechniques used in preparing the data, and later we will present the implementations\nof the techniques.\nThe first step is to parse sentences and phrases into formats that are more compatible\nwith the model by tokenization. This is the process by which we discretize a particular\nEnglish or French sentence into its constituent tokens. For instance, a simple word-\nlevel tokenizer will consume the sentence \u201cI read.\" to produce the array [\"I\", \u201cread,\n\"\"], or it would consume the French sentence \u201cJe lis.\u201d to produce the array [\"Je\u201d, \"lis\u201d\n\" ]. A character-level tokenizer may break the sentence into individual characters or\ninto pairs of characters like [\"I\",\"\n-oadsai 'I.. ape, &o, ', I) pue I. aP, 'e, 'o, .\ntively. One kind of tokenization may work better than the other, and each has its pros\nand cons. For instance, a word-level tokenizer will ensure that the model produces\nwords that are from some dictionary, but the size of the dictionary may be too large\nto efficiently choose from during decoding. This is in fact a known issue and some-\n12This code can be found at:https://github.com/tensorflow/tensorflow/tree/ro.7/tensorflow/models/rnn/translate.\n194  Chapter 7:Models for Sequence Analysis",
        "appended EOS tokens to our target sequences.\nI\npeas\n<PAD>\n<PAO>\n<PAD>\n<PAD>\nje\nlis\n(EOS>\n(PAD)\n<PAD>\nKPAGY\nSee\nyou\n1n\nlittle\nhile\ntout\n1'heure\nA\n4EOS>\n(PAD>\n(PAO)\nFigure 7-28. Naive strategy for padding sequences\nThis step saves us the trouble of having to construct a diffrent seq2seq model for\neach pair of source and target lengths. However, this introduces a diffrent issue: if\nthere were a very long sequence, it would mean that we would have to pad every\nother sequence up to that length. This would make a short sequence padded to the\nend take as much computational resources as a long one with few PAD tokens, which\nis wasteful and could introduce a major performance hit to our model. We could con-\nDissectinga Neural Translation Network|195",
        "the coresponding translations.This is where bucketing helps us.\nas shown in Figure 7-29.\nread\n<PAD>\n11s\n<EOS>\nBucket 1\nJe\nlittle\nin\nwhile\nSee\nyou\nl'heure\n<EOS>\n<PAD>\nBucket j\ntout\n(PAD)\nFigure 7-29. Padding sequences with buckets\nways that allow even further GPU efficiency.\nWith the sequences properly padded, we need to add one additional token to the tar-\nget sequences: a GO token. This GO token will signal to the decoder that decoding\nneeds to begin, at which point it will take over and begin decoding.\nThe last improvement we make in the data preparation side is that we reverse the\nhas become a standard trick to try when training neural machine translation models.\nThis is a bit of an engineering hack, but consider the fact that our fixed-size neural\nstate can only hold so much information, and information encoded while processing\nthe beginning of the sentence may beoverwriten while encoding later parts of the\n196| Chapter7:Models for Sequence Analysis",
        "(PAD)\n(PAD)\nread\nJe\nlis\n<G0>\nc05\nBucket 1\nwhlle\nlittle\nln\ntout\n500\n<GO>\nl'heure\n\u5c0f\u7684\nBucket j\nets and reversing the inputs:\nsume:\nfor _ in xrange(self.batch_size):\nbucket_id])\n# Encoder inputs are padded and then reversed.\nencoder_pad = [data_utlls.PAD_ID] * (encoder_stze - len(\nencoder_(nput))\nencoder _inputs.append(list(reversed(encoder_input +\nencoder_pad)))\n# Decoder inputs get an extra \"Go\u201d symbol,\n# and are then padded.\ndecoder_pad_size = decoder_size -len(decoder_input) -1\ndecoder_inputs.append([data_utils.co_ID] + decoder_input +\n[data_utils.PAD_ID]\ndecoder_pad_stze)\n# above.\n[], [], []\nDisseting a Neural Transiation Networt|197",
        "for length_idx in xrange(encoder_size):\nbatch_encoder_inputs.append(\nnp.array([encoder_inputs[batch_idx][length_idx]\nfor batch_idx in xrange(self.batch_stze)],\ndtype=np.tnt32))\n# Batch decoder inputs are re-indexed decoder_inputs,\n#we create weights.\nfor length_idx in xrange(decoder_size):\nbatch_decoder_inputs.append(\nnp.array([decoder_inputs[batch_idx][length_ldx]\nfor batch_idx in xrange(self.batch_size)],\ndtype=np.int32))\ndefined placeholders into the correct shape:\n# Create target_weights to be 0 for targets that\n#are padding.\nbatch_weight = np.ones(self.batch_size, dtype=np.float32)\nfor batch_idx in xrange(self.batch_size):\n#We set weight to  if the corresponding target is\n# a PAD symbol.\n#The corresponding target is decoder_input shifted\n# by 1 forward.\nif length_idx< decoder_size -1:\ntarget = decoder_inputs[batch_idx][length_idx + 1]\nif length_idx == decoder_size - 1 or\ntarget == data_utils.PAD_ID:\nbatch_weight[batch_idx] = 0.0\nbatch_weights.append(batch_weight)\nreturn batch_encoder_inputs, batch_decoder_inputs,\nbatch_weights\nFinally, we set the target weights of zero to those tokens that are simply the PAD\ntoken.\nWith the data preparation now done, we are ready to begin building and training our\nmodell We first detail the code used during training and test time, and abstract the\nmodelwayWhdnanmakuewertadthhigl\npipeline, andwell tn studyh qseqmdel inredthaways hist\nstep during training is to load our data:\ndef tratn():\n#Prepare WMT data.\nprint(\"Preparing WMT data tn %s\"% FLAGS.data_dir)\nen_train,fr_train, en_dev, fr_dev,-,_=\ndata_utils.prepare_wmt_data(\nFLAGS.data_dir,FLAGS.en_vocab_sze,FLAGS.fr_vocab_size)",
        "with tf.session() as sess:\n# Create model.\nFLAGS.stze)\nties:\n% FLAGS.max_train_data_size)\ndev_set = read_data(en_dev, fr_dev)\ntrain_set = read_data(en_train, fr_train,\nFLAGS.max_train_data_size)\ntrain_bucket_sizes = [len(train_set[b]) for b in xrange(\nlen(_buckets))]\ntrain_total_size = float(sum(train_bucket_sizes))\n# A bucket scale is a list of increasing nunbers\n# from O to 1 that we'll use to select a bucket.\n# Length of [scale[i], scale[i+1]] is proportional to\n# the size if i-th training bucket, as used later.\ntrain_buckets_scale =[sum(train_bucket_sizes[:i+1]) /\ntrain_total_size\nfor i in xrange(len\uff08\ntrain_bucket_sizes))]\ndoo sa azn m do uun ueu ino iu mou am Xpeai eep m\nvariables like current_step and previous_lossestoOor empty It is important t\ndata:\n# This is the training loop.\nstep_time, loss = 0.0, 0.0\ncurrent_step = 0\nprevious_losses =[]\nwhile True:\n# We pick a random number\n# in tratn_buckets_scale.\nDissetinga Neural ranslation Network|199",
        "random_number_01 = np.random.random_sample()\nbucket_ld = min([i for i in xrange(len(\ntrain_buckets_scale))\nif train buckets_scale[i] >\nrandom_number_01])\n# Get a batch and make a step.\nstart_time = time.time()\nmodel.get_batch(\ntratn_set, bucket_id)\ndecoder_inputs,\ntarget_welghts, bucket_id,\nFalse)\nrunning metrics:\nstep_time += (time.time() - start_time) /\nFLAGS.steps_per_checkpoint\nloss += step_loss / FLAGS.steps_per_checkpoint\ncurrent_step +=1\n# Once in a whtle, we save checkpoint, print statistics,\n#and run evals.\nif current _step % FLAGS.steps_per_checkpoint ==0:\n#Print statistics for the previous epoch.\nperplexity = math.exp(float(loss)) if loss <\n300 else float(\"inf\")\nprint (\"global step %d learning rate %.4f\nstep-time %.2f perplexity\n\"%.2f\"%(model.global _step.eval(),\nmodel.learning_rate.eval(),\nstep_time,perplexity))\n#Decrease learning rate if no improvement was seen over\n#last3 times,\nif len(previous_losses)>2 and loss>max(\nprevtous_losses[-3:]):\nsess.run(model,learning_rate_decay_op)\nprevious_losses,append(loss)\n#Save checkpoint and zero timer and loss.\ncheckpoint_path =os.path.join(FLAGS.train_dtr,\n\"translate,ckpt\")\nmodel,saver,save(sess, checkpoint_path,",
        "global_step=model.glctal se)\n# their perplexity.\nif len(dev_set[bucket_id]) == 0:\ncontinue\nencoder_inputs, decoder_inputs,\ntarget_weights = model.get_batch(\ndev_set,bucket_id)\nencoder_inputs, decoder_inputs,\n, eval_loss, _ = model.step(sess, encoder_Lrouts,\ndecoder_inputs,\ntarget_weights,\nbucket_id,\nTrue)\neval_ppx = math.exp(float(eval_loss)) if evat_loss \n300 else float\uff08\n\"inf\")\nprint(\"eval: bucket %d perplexity %.2f*3(\nbucket_id,eval_ppx))\nsys.stdout.flush()\nWe also have another major use case for our model single-use preadiction. la cther\nwe, or other users,provide.To do so,we use the decode() methed This methodwill\nessentially carry out the same functions as was done in the evaluation locp for the\nheld-out development set.However, the largest difference is that during training and\nevaluation, we never needed the model to translate the output embeddings to output\ntokens that are human-readable, which is something we do here. We detail this\nmethod now.\npoint step:\ndef decode():\nwith tf.Sesston() as sess:\nDissectinga Neural Translation Network|201",
        "#Check if the sizes match.\nencoder_size, decoder_size = self.buckets[bucket_id]\nif len\uff08encoder_inputs) != encoder_size:\nraise ValueError(\"Encoder length must be equal to the one\nin bucket,\"\n\"%d != %d.\" % (len(\nencoder_inputs),encoder_size))\nif len\uff08decoder_inputs) != decoder_size:\nraise ValueError(\"Decoder length must be equal to the one\nin bucket,\"\n\" %d != %d.\" % (len(decoder_inputs),\ndecoder_size))\nif len(target_weights) != decoder_size:\nraise ValueError(\"Weights length must be equal to the one\nin bucket,\"\n\"%d != %d.\"%(len(target_weights)\no\ndecoder_size))\n0#\n# Input feed: encoder inputs, decoder inputs, target_weights,\n#as provided.\ninput_feed =[}\nfor l in xrange\uff08encoder_size):\ninput_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\nbuto\nfor L in xrange(decoder_size):\ninput_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\ninput_feed[self.target_weights[l].name] = target_weights[l]\n# Since our targets are decoder inputs shifted by one,\n# we need one more.\nlast_target = self.decoder_inputs[decoder_size].name\ninput_feed[last_target]= np.zeros([self.batch_size],\ndtype=np.int32)\nThe output feed, if a loss is computed and needs to be backpropagated through the\nand computes the gradient norm and loss for the batch:\n# Output feed: depends on whether we do a backward step or\n#not.\nif not forward_only:\noutput_feed =[self.updates[bucket_id],# Update Op that\n#does SGD.\nself.gradient_norms[bucket_id], # Gradient\n#norm.\nself.losses[bucket_id]] # Loss for this\n#batch.\nelse:\noutput_feed= [self.losses[bucket_id]] # Loss for this\n#batch.\nfor l in xrange(decoder_size): # Output logits.\noutput_feed.append(self.outputs[bucket_id][l])\n204Chapter 7:Models forSequence Analysis",
        "puts are returned for decoding purposes:\noutputs = sesston.run(output_feed,lnput_feed)\nif not forward_only:\nreturn outputs[1], outputs[2], None #, attns\n# Gradient norm, loss, no outputs.\nelse:\nreturn None, outputs[e],outputs[1:]#,attns\n# No gradient norm,loss, outputs.\nstructor.\nThe create_model() method itself is fairly straightforward it uses a number of use-\ndefined or default flags, such as the sizes of the English and French vocabularies ad\nbatch size, to create the model by using the constructor seqzseq_model.Seq2Sece\ndel. One particularly interesting flag is the use_fp16 flag. With this, a lower preciscn\nis used as the type in the underlying numpy arrays this results in faster perormance\nat the cost of some amount of precision. However,its often the case that 16-bit repre-\nsentations are sufficient for representing losses and gradient updates and ofen pe-\nform close to the level of using 32-bit representations. Model creaficn can be\nachieved using the following code:\ndef create_model(session, forward_only):\n\"\"\"Create translation model and initialize or\nload parameters in session.\"\"\"\ndtype = tf.float16 if FLAGs.use_fp16 else tf.float32\nmodel = seq2seq_model.Seq2SeqModel(\nFLAGS.en_vocab_size,\nFLAGS.fr_vocab_size,\n_buckets,\nFLAGS.size,\nFLAGS.num_layers,\nFLAGS.max_gradient_norm,\nFLAGS.batch_size,\nFLAGs.learning_rate,\nFLAGs.learning_rate_decay_factor,\nforward_only=forward_only,\ndtype=dtype)\nBefore returning the model,a check is done to see if there are any previously check-\npointed models from earlier training runs. If so, this model and its parameters are\nread into the model variable and used. This allows us to stop training at a checkpoint\nand later resume it without training from scratch.Otherwise, the fresh model created\nis returned as the main object:\nDissetingaNeuralTrnslation Network|205",
        "ckpt = tf.tratn.get_checkpotnt_state(FLAGS.train_dir)\nif ckpt and tf.train.checkpoint_exists(\nckpt.model_checkpoint_path):\nprint(\"Reading model parameters from %s\"\n%ckpt.model_checkpotnt_path)\nmodel.saver.restore(sesston, ckpt.model_checkpoint_path)\nelse:\nprint(\"Created model with fresh parameters.\")\nsession.run(tf.global_variables_initializer())\nreturn model\nthe code and sketch the details of the overarching computation graph.\nfew class-level fields are created:\nclass Seq2SeqModel(object):\ndef_init_(self,\nsource_vocab_size,\ntarget_vocab_size,\nbuckets,\nsize,\nnum_layers,\nmax_gradient_norm,\nbatch_size,\nlearning_rate,\nlearning_rate_decay_factor,\nuse_lstm=False,\nnum_samples=512,\nforward_only=False,\ndtype=tf.float32):\nself.source_vocab_size = source_vocab_size\nself.target_vocab_size = target_vocab_size\nself.buckets = buckets\nself.batch_size = batch_size\nself.learning_rate = tf.Variable(\nT.20\nfloat(learning_rate),tratnable=False,dtype=dtype)\nself.learning_rate_decay_op = self.learning_rate.assign(\n12010\nself.learning_rate * learning_rate_decay_factor)\n.2007\nself.global_step=tf.Vartable(0, trainable=False)\nThe next part creates the sampled sofmax and the output projection This is an\nimprovement over basic seq2seq models in that they allow for efficient decoding over\nds pa u  s ndino  jod pue senqeoa jdino sre\n206|Chapter 7:Modelsfor Sequence Analysis",
        "# vocabulary size.\nself.target_vocab_size:\nstze],dtyre=dtyre)\nw = tf.transpose(w_t)\ndtype=dtype)\noutput_projection = (w, b)\nlocal_b = tf.cast(b, tf.float32)\nreturn tf.cast(\ntf.n.sampled_softmax_loss(local_w_t, local_b,\nlocal_inputs, labels,\nnum_samples,\nself.target_vocab_size),\ndtype)\nsoftmax_loss_function = sampled_loss\n sgp a  a pe un on ns an are au inq 'so \nfaster:\n#Create the internal multi-layer cell for our RNN.\nsingle_cell = tf.nn.rnn_cell.GRucell(size)\nif use_lstm:\nsingle_cell = tf.nn.rnn_cell.BasicLSTMcell(size)\ncell = single_cell\nif num_layers >1:\ncell = tf.nn.rnn_cell.MultiRNNCell([single_cell] *\nnum_layers)\ntlon_seq2seq(), which we will discuss later:\n#input and attention.\nencoder_inputs,\ndecoder_inputs,\nDissectinga Neural Translation Network|207",
        "cell,\nnum_encoder_symbols=source_vocab_size,\nnum_decoder_symbols=target_vocab_size,\nembedding_size=size,\noutput_projection=output_projection,\nfeed_previous=do_decode,\ndtype=dtype)\nWe define placeholders for the inputs and targets:\n# Feeds for inputs.\nself.encoder_inputs =[]\nself.decoder_inputs =[]\nself.target_weights=[]\nfor i in xrange(buckets[-1][o]): # Last bucket is\n# the biggest one.\nself.encoder_inputs.append(tf.placeholder(tf.int32,\nshape=[None],\nname=\"encoder{o}\".format(i)))\nfor i in xrange(buckets[-1][1] + 1):\nself.decoder_inputs.append(tf.placeholder(tf.int32,\nshape=[None],\nname=\"decoder{0}\".format(i)))\nself.target_weights.append(tf.placeholder(dtype,\nCE.00.25\nshape=[None],\nname=\"weight{o}\".format(i)))\n# Our targets are decoder inputs shifted by one.\ntargets =[self.decoder_inputs[i + 1]\nfor i in xrange(len(self.decoder_inputs) - 1)]\nWe\nnow\ncompute  the\noutputs\nand  losses\nfrom\nthe\nfunction\nexample sequence or as a weighted cross-entropy loss for a sequence of logits:\n# Training outputs and losses.\nif forward_only:\nself.outputs, self.losses = seq2seq.model_with_buckets(\nself.encoder_inputs, self.decoder_inputs, targets,\nself.target_weights, buckets, lambda x, y:\nseq2seq_f(x, y, True),\nsoftmax_loss_function=softmax_loss_function)\n# If we use output projection, we need to project outputs\n#for decoding.\nif output_projection is not None:\nfor b in xrange\uff08len(buckets)):\nself.outputs[b] =[\ntf.matmul(output, output_projection[o]) +\noutput_projection[1]\nfor output in self,outputs[b]\n1\nelse:\n208 |Chapter 7:Models for Sequence Analysis",
        "if not forward_only:\nself.gradient_norms = []\nself.updates =[]\ntug ate\nself.learning_rate)\nfor b in xrange(len(buckets)):\ngradients,\nmax_gradient_ncrm)\nself.gradient_norms.append(norm)\nself.updates.append(opt.apply_gradients(\nzip(clipped_gradients, params), glcbal _steg=\nself.glcbal_step))\nself.saver = tf.train.Saver(tf.all_ variables())\ntion_seq2seq().\nWhen initializing this model, several flags and arguments are passed as function\narguments. One argument of particular note is feed _previous. When this is true,the\ndecoder will use the outputted logit at time step T as input to time step Ttl. In this\nway, it is sequentially decoding the next token based on all tokens thus far. We can\ndescribe this type of decoding, where the next output depends on allprevious out-\nputs, as autoregressive decoding.\ndef embedding_attention_seqzseq(encoder_inputs,\ndecoder_inputs,\ncell,\nnum_encoder_symbols,\nnum_decoder_symbols,\nembedding_size,\noutput_projection=None,\nfeed_previous=False,\ndtype=None,\nDissectinga Neural Translation Network|209",
        "scope=None,\nintttal_state_attention=False):\nWe first create the wrapper for the encoder.\nwith varlable_scope.variable_scope(\nas scope:\ndtype = scope.dtype\nencoder_cell = rnn_cell.Embeddingwrapper(\ncell,\nembedding_classes=num_encoder_symbols,\nembedding_size=embedding_size)\nencoder_outputs, encoder_state = rnn.rnn(\nencoder_cell, encoder_inputs, dtype=dtype)\nas a distribution:\n# First calculate a concatenation of encoder outputs\n#to put attention on.\ntop_states =[\narray_ops.reshape(e, [-1, 1, cell.output_size]) for e\nin encoder_outputs\n1\nattention_states = array_ops.concat(1, top_states)\nwrapped to be one that uses an output projection:\noutput_size = None\nif output_projection is None:\ncell=rnn_cell.outputProjectionwrapper(cell,\nnum_decoder_symbols)\noutput_size = num_decoder_symbols\nFrom here, we compute the outputs and states using the embedding_atten\ntion_decoder:\nif isinstance(feed_previous, bool):\nreturn embedding_attention_decoder(\ndecoder_inputs,\nencoder_state,\nattention_states,\ncell,\nnum_decoder_symbols,\nenbedding_size,\noutput_size=output_size,\noutput_projection=output_projection,\nfeed_previous=feed_previous,\ninitial_state_attention=initial_state_attention)\n210|Chapter7:ModelsforSequence Analysis",
        "invoked in this step:\ntnttial_state,\nattention_states,\ncell,\nnum_symbols,\nembedding_stze,\noutput_size=None,\noutput_projection=None,\nfeed_previous=False,\nupdate_embedding_for_previcus=\nTrue,\ndtype=None,\nscope=None,\ninitial_state_attention=False):\nif output_size is None:\noutput_size = cell.output_size\nif output_projection is not None:\ndtype=dtype)\n[num_symbols])\nwith variable_scope.variable_scope(\nscope or \"embedding_attention_decoder\", dtype=dtype)\nas scope:\nembedding = variable_scope.get_variable(\"embedding\",\n[num_symbols,\nembedding_size])\nloop_function = _extract_argmax_and_embed(\nembedding,output_projection,\nupdate_embedding_for_previous) if feed_previous\nelse None\nemb_inp =[\nembedding_ops.embedding_ lookup(embedding, i) for i in\ndecoder_inputs\nreturn attention_decoder(\nemb_inp,\ninitial_state,\nattention_states,\ncell,\noutput_size=output_size,\nDissecting a Neural Translation Network| 211",
        "loop_function=loop_function,\nintial_state_attention=inttial_state_attention)\nthe hidden features to the right size:\ndef attention_decoder(decoder_inputs,\ninitial_state,\nattention_states,\ncell,\noutput_size=None,\nloop_function=None,\ndtype=None,\nscope=None,\ninitial_state_attention=False):\nif not decoder_inputs:\ndecoder.\")\nif attention_states.get_shape()[2].value ts None:\nraise ValueError(\"Shape[2] of attention_states must be known:\n%s\"%\nattention_states.get_shape())\nif output_size is None:\noutput_size =cell.output_size\nwith variable_scope.variable_scope(\nscope or \u201cattention_decoder\", dtype=dtype) as scope:\ndtype = scope.dtype\nbatch_size = array_ops.shape(decoder_inputs[0])[o] # Needed\n#for\n#reshaping.\nattn_length = attention_states.get_shape()[1].value\nif attn_length is None:\nattn_length =array_ops.shape(attention_states)[1]\nattn_size = attention_states.get_shape()[2].value\n# To calculate W1 *h_t we use a 1-by-1 convolution,\n#need to reshape before.\nhidden = array_ops.reshape(attention_states,\n[-1, attn_length, 1, attn_size])\nhidden_features =[]\nv=[]\nattention_vec_size = attn_size # Size of query vectors\nfor attention.\nk = variable_scope.get_variable(\"AttnW_o\",\n[1,1, attn_size,\nattention_vec_size])\nhtidden_features.append(nn_ops,conv2d(hidden, k,\n[1, 1, 1, 1], \"SAME\"))\n212 Chapter7Models for Sequence Analysis",
        "v.append(\n[attentton_vec_stze]))\nstate = intttal_state\ndef attention(query):\nand query.\"\"\"\n# stored here.\n#flatten it.\n#specified.\nndims = q.get_shape().ndims\nif ndims:\nassert ndims == 2\nquery = array_ops.concat(1, query_list)\n# query = array_ops.concat(query_list, 1)\ny = array_ops.reshape(y, [-1, 1, 1,\nattention_vec_size])\n# Attention mask is a softmax of v<T * tanh(...).\ns = math_ops.reduce_sum(v[e] *math_ops.tanh(\nhidden_features[e] +y),\n[2,3]]\na = nn_ops.softmax(s)\n# Now calculate the attention-weighted vector d.\nd= math_ops.reduce_sum(\narray_ops.reshape(a, [-1, attn_length, 1, 1])\nhidden,[1,2])\nds.append(array_ops.reshape(d, [-1, attn_size]))\nreturn ds\nwith the initial state:\noutputs = []\nprev = None\n# vectors is set.\na.set_shape([None, attn_size])\nif initial_state_attention:\n(ese)oae = se\nDissecting a Neural Translation Network | 213",
        "to the same dynamics:\nfor i, inp in enumerate(decoder_inputs):\nift>o:\n# If loop_function is set, we use it instead of\n#decoder_inputs.\nif loop_function is not None and prev is not None:\nwith variable_scope.variable_scope(\"loop_function\",\nreuse=True):\ninp =loop_function\uff08prev,t)\n# Merge input and prevlous attentions into one vector of\n#the right size.\ninput_size = inp.get_shape().with_rank(2)[1]\nif input_size.value is None:\nraise ValueError(\"could not infer input size from input:\n%s\"%inp.name)\nx=linear\uff08[inp] + attns,input_size,True)\n#Run the RNN.\ncell_output, state = cell\uff08x,state)\n#Run the attention mechanism.\nif i==O and initial state_attention:\nwith variable_scope.variable_scope(\nvariable_scope.get_variable_scope(), reuse=True):\nattns=attention(state)\nelse:\nattns = attention(state)\nwith variable_scope.variable_scope(\n\"AttnoutputProjection\"):\noutput = linear([cell_output] + attns, output_size,\nTrue)\nif loop_function is not None:\nprev=output\noutputs.append(output)\nreturn outputs, state\nWith this, weve successfully completed a fulltour of the implementation details of a\nfairly sophisticated neural machine translation system. Production systems have\ncompute servers to ensure that state-of-the-art performance is met.\nthe learning rate anneal over time as well.\n214| Chapter 7:Models for Sequence Analysis",
        "PerptexryTorErgisnn Frercn Tmanon\n4OO\n200\n8\n600\n400\n15k\n20k\n25k\n30k\n10k\n5k\nEpochs\ntion system.\nLearming Rate Decay\n100k\n150k\n200k\n250k\n30Ck\n50k\nEpochs\ntraining, the model was approaching a stable state.\nDissectinga Neural Translation Network |215",
        "when translating the i token in the French sentence.\nplaced on that element.\nWe can immediately see that the atention mechanism seems to be working quite\nwell. Large amounts of attention are generally being placed in the right areas, even\nthough there is slight noise in the model's prediction. It is possible that adding addi-\ntional layers to the network would help produce crisper attention. One impressive\naspect is that the phrase \u201cthe European Economic\u201d is translated in reverse in French\nas the \u201czone \u00e9conomique europeenne,\u201d and as such, the attention weights reflect this\nflip! These kinds of attention patterns may be even more interesting when translating\nfrom Englshtoadifferent language that does not parse smoothly from left toright.\nworks and begin a foray into more sophisticated learning.\n216|Chapter7:Models for Sequence Analysis",
        "Summary\ntranscription.\nSummary 1  217"
    ],
    "metadata": [
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 0
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 1
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 2
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 3
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 4
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 5
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 6
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 7
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 8
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 9
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 10
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 11
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 12
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 13
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 14
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 15
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 16
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 17
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 18
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 19
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 20
        },
        {
            "source": "/Users/spencerpresley/UMBC-HACKATHON/UMBC-2024-Hackathon/pythonBackend/files/ExampleFileForDev.pdf",
            "page": 21
        }
    ]
}