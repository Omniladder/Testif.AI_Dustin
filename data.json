{
    "data": "CHAPTER4\nBeyond Gradient Descent\nFor several years, researchers resorted to layer-wise greedy pre-training in order to\ntime-intensive strategies would try to find more accurate initializations for the mod-\nel's parameters one layer at a time before using mini-batch gradient descent to con-\nverge to the optimal parameter settings. More recently, however, breakthroughs in\noptimization methods have enabled us to directly train models in an end-to-end fash-\nion.\nIn this chapter, we will discuss several of these breakthroughs. The next couple of sec-\ntions will focus primarily on local minima and whether they pose hurdles for success-\nfully training deep models. In subsequent sections, we will further explore the\n nonconvex error surfaces induced by deep models, why vanilla mini-batch gradient\ncessing Systems 19 (2007): 153.\n63Local Minima in the Error Surfaces of Deep Networks\nand global structure. Take the following analogy as an example.\nconvex) and we were smart about our learning rate, we could use the gradient descent\nalgorithm to eventually find the bottom of the bowl. But the surface of the US is\nextremely complex, that is to say, is a nonconvex surface, which means that even if we\nfind a valley (a local minimum), we have no idea if it's the lowest valley on the map\n(the global minimum). In Chapter 2, we talked about how a mini-batch version of\ngradient descent can help navigate a troublesome eror surface when there are spuri-\nous regions of magnitude zero gradients. But as we can see in Figure 4-1, even a sto-\nchastic error surface won't save us from a deep local minimum.\nE\nbg nitgo6\u5c71o\nW,\nFigure 4-1. Mini-batch gradient descent may aid in escaping shallow local minirma, but\noften fails when dealing with deep local minima, as shown\nNow comes the critical question. Theoretically, local minima pose a significant\nissue. But in practice, how common are local minima in the error surfaces of deep\nnetworks? And in which scenarios are they actually problematic for training? In the\nfollowing two sections, we'll pick apart common misconceptions about local minima.\n64| Chapter 4:Beyond Gradient DescentModel I dentifiability\n2805\nconfigurations.\nfigurations due to symmetry\nModelldentifiability   65"
}